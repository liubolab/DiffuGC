{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 20)\n",
      "ts_target: 0\n",
      "{'overall': {'trainable': '34.37M', 'non_trainable': '0', 'total': '34.37M'}, 'denoise_Transformer': {'trainable': '34.37M', 'non_trainable': '0', 'total': '34.37M'}}\n",
      "lorenz_p20_t300_f40: start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2|train loss: 68899.8203|l_recon: 68849.2871|ridge_loss: 50.5357|l2_loss:26909.9717|lr: 0.000020|prox_lam:5.00:   0%|          | 2/2000 [00:20<5:45:55, 10.39s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 104\u001b[0m\n\u001b[1;32m    102\u001b[0m model \u001b[38;5;241m=\u001b[39m instantiate_from_config(configs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    103\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(config\u001b[38;5;241m=\u001b[39mconfigs, args\u001b[38;5;241m=\u001b[39margs, model\u001b[38;5;241m=\u001b[39mmodel, dataloader\u001b[38;5;241m=\u001b[39mtrainloader,weight_decay\u001b[38;5;241m=\u001b[39mconfigs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],logger\u001b[38;5;241m=\u001b[39mgc_logger)\n\u001b[0;32m--> 104\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts_target\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/0shared/liubo/diffusion-gc/DiffuGC/engine/solver.py:211\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, ts_target)\u001b[0m\n\u001b[1;32m    209\u001b[0m     best_loss \u001b[38;5;241m=\u001b[39m l2_loss\n\u001b[1;32m    210\u001b[0m     early_stop_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_best\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_best\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     early_stop_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/data/0shared/liubo/diffusion-gc/DiffuGC/engine/solver.py:87\u001b[0m, in \u001b[0;36mTrainer.save_best\u001b[0;34m(self, norm)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_best\u001b[39m(\u001b[38;5;28mself\u001b[39m,norm):\n\u001b[1;32m     80\u001b[0m     data\u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep,\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     86\u001b[0m     }\n\u001b[0;32m---> 87\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults_folder\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/bes_model.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/0shared/liubo/anaconda/envs/gc/lib/python3.10/site-packages/torch/serialization.py:618\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    615\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/data/0shared/liubo/anaconda/envs/gc/lib/python3.10/site-packages/torch/serialization.py:466\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname('__file__'), '../'))\n",
    "\n",
    "from engine.solver import Trainer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from engine.logger import Logger\n",
    "from Utils.io_utils import load_yaml_config, instantiate_from_config\n",
    "from Models.interpretable_diffusion.model_utils import normalize_to_neg_one_to_one, unnormalize_to_zero_to_one\n",
    "\n",
    "\n",
    "class get_dataset(Dataset):\n",
    "    def __init__(self, data, seq_length,mode,train_split=0.8):\n",
    "        super(get_dataset, self).__init__()\n",
    "\n",
    "        self.samples = data\n",
    "        \n",
    "        self.seq_length=seq_length\n",
    "        self.pred_length = 0\n",
    "        self.features = data.shape[-1]\n",
    "        \n",
    "        self.data = self.get_data(data)\n",
    "        train_num = int(train_split * len(self.data))\n",
    "        if mode == 'train':\n",
    "            self.data = self.data[:train_num, :, :]\n",
    "        else:\n",
    "            self.data = self.data[train_num:, :, :]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index, :, :]\n",
    "                \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get_data(self,data):\n",
    "\n",
    "        # data_max = np.max(data, axis=0)\n",
    "        # data_min = np.min(data, axis=0)\n",
    " \n",
    "        # data = (data - data_min) / (data_max - data_min)\n",
    "        num_sample = len(data) - self.seq_length - self.pred_length + 1\n",
    "        seq_data = torch.zeros(num_sample,\n",
    "                               self.seq_length + self.pred_length,\n",
    "                               self.features)\n",
    " \n",
    "        #         print(data.iloc[0:0 + self.seq_length + 1, self.features].values)\n",
    " \n",
    "        for i in range(num_sample):\n",
    "            seq_data[i] = torch.tensor(data[i:i + self.seq_length + self.pred_length,\n",
    "                                       :])\n",
    "        #         print(data_max)\n",
    "        #         print(data_min)\n",
    " \n",
    "        return seq_data\n",
    "\n",
    "p=20\n",
    "t=300\n",
    "f=40\n",
    "data_name=f'lorenz_p{p}_t{t}_f{f}'\n",
    "data= np.load(f'/data/0shared/liubo/diffusion-gc/DiffuGC/my_exp/{data_name}/data.npy')\n",
    "print(data.shape)   \n",
    "\n",
    "train_split=0.8\n",
    "batch_size=64\n",
    "\n",
    "train = data[:int(train_split*data.shape[0]), :]\n",
    "test = data[int(train_split*data.shape[0]):,:].reshape(1, -1, data.shape[-1])\n",
    "\n",
    "\n",
    "class Args_Example:\n",
    "    def __init__(self) -> None:\n",
    "        self.name=data_name\n",
    "        self.config_path = '/data/0shared/liubo/diffusion-gc/DiffuGC/Config/'+data_name+'.yaml'\n",
    "        self.save_dir = '/data/0shared/liubo/diffusion-gc/DiffuGC/my_exp/'+data_name\n",
    "        self.gpu = 6\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "args =  Args_Example()\n",
    "configs = load_yaml_config(args.config_path)\n",
    "device = torch.device(f'cuda:{args.gpu}' if torch.cuda.is_available() else 'cpu')\n",
    "train_dataset = get_dataset(train, seq_length=configs['model']['params']['seq_length'], mode='train', train_split=train_split)\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "log_name= 'seq_length'+str(configs['model']['params']['seq_length'])+'_batch_size'+str(batch_size)+'_lr'+str(configs['solver']['base_lr'])+'_d_model'+str(configs['model']['params']['d_model'])+'_num_layers'+str(configs['model']['params']['n_layer_enc'])+'_num_heads'+str(configs['model']['params']['n_heads'])+'_dropout'+str(configs['model']['params']['attn_pd'])\n",
    "feature_size=data.shape[1]\n",
    "ts_target=0\n",
    "print(f\"ts_target: {ts_target}\")\n",
    "for seq_length in [24,48,96]:    \n",
    "\n",
    "    print(f'-------{seq_length}-------')\n",
    "    configs['model']['params']['seq_length'] = seq_length\n",
    "    for d_model in [256,512]:\n",
    "        configs['model']['params']['d_model'] = d_model\n",
    "        gc_logger = Logger(args=args,name=log_name,ts_target=ts_target)\n",
    "        model = instantiate_from_config(configs['model']).to(device)\n",
    "        trainer = Trainer(config=configs, args=args, model=model, dataloader=trainloader,weight_decay=configs['solver']['weight_decay'],logger=gc_logger)\n",
    "        trainer.train(ts_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
